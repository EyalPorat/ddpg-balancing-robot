# Enhanced DDPG Training Configuration with Dual Prioritized Replay

# Model parameters
model:
  actor:
    hidden_dims: [10, 10]
    learning_rate: 0.0001
    
  critic:
    hidden_dims: [256, 128, 64]
    learning_rate: 0.0001

# Training parameters
training:
  total_episodes: 450
  max_steps_per_episode: 150
  batch_size: 512
  gamma: 0.99  # Discount factor
  tau: 0.005   # Target network update rate
  
  # Experience replay settings
  buffer_size: 1000
  min_buffer_size: 500  # Minimum samples before training starts
  
  # Dual Prioritized Replay Buffer settings
  use_critic_prioritization: true
  use_actor_prioritization: true
  critic_alpha: 0.6  # Priority exponent for critic (higher = more prioritization)
  actor_alpha: 0.6   # Priority exponent for actor
  critic_beta_start: 0.4  # Initial importance sampling correction for critic
  actor_beta_start: 0.4   # Initial importance sampling correction for actor
  beta_anneal_steps: 500  # Steps to anneal beta from start value to 1.0
  
  # Actor prioritization method
  # Options: 'q_value', 'advantage', 'policy_gradient'
  actor_priority_method: "q_value"
  advantage_samples: 10  # Number of random samples for advantage calculation
  
  # Buffer management
  shared_transitions: true  # Whether to store all transitions in both buffers
  
  # Exploration settings
  action_noise: 0.1
  noise_decay: 0.9999
  min_noise: 0.01

  # Evaluation settings
  eval_frequency: 10
  eval_episodes: 5
  save_frequency: 100

  # Curriculum learning settings
  use_curriculum: false
  curriculum_epochs: 200
  curriculum_initial_angle_range_precent: 0.1
  curriculum_initial_angular_velocity_range_precent: 0.1

# Logging parameters
logging:
  log_frequency: 1
  video_frequency: 100
  checkpoint_frequency: 100
  tensorboard: true
