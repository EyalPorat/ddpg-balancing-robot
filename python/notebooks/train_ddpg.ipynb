{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DDPG Training for Balancing Robot\n",
                "\n",
                "This notebook trains a DDPG agent for the balancing robot environment using the provided configurations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import yaml\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from src.balancing_robot.models import Actor, Critic, ReplayBuffer, SimNet\n",
                "from src.balancing_robot.environment import BalancerEnv\n",
                "from src.balancing_robot.training import DDPGTrainer\n",
                "from src.balancing_robot.visualization import plot_training_metrics, create_episode_animation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Configurations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load DDPG and environment configurations\n",
                "with open('../configs/ddpg_config.yaml', 'r') as f:\n",
                "    ddpg_config = yaml.safe_load(f)\n",
                "\n",
                "with open('../configs/env_config.yaml', 'r') as f:\n",
                "    env_config = yaml.safe_load(f)\n",
                "\n",
                "with open('../configs/simnet_config.yaml', 'r') as f:\n",
                "    simnet_config = yaml.safe_load(f)\n",
                "\n",
                "# Create log directory\n",
                "log_dir = Path('logs/ddpg_training')\n",
                "log_dir.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Environment and Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
                        "  gym.logger.warn(\n",
                        "c:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
                        "  gym.logger.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Actor(\n",
                        "  (network): Sequential(\n",
                        "    (0): Linear(in_features=6, out_features=8, bias=True)\n",
                        "    (1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
                        "    (2): ReLU()\n",
                        "    (3): Linear(in_features=8, out_features=8, bias=True)\n",
                        "    (4): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
                        "    (5): ReLU()\n",
                        "  )\n",
                        "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
                        ")\n",
                        "Critic(\n",
                        "  (l1): Linear(in_features=7, out_features=256, bias=True)\n",
                        "  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                        "  (hidden_layers): Sequential(\n",
                        "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
                        "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                        "    (2): ReLU()\n",
                        "  )\n",
                        "  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "# Set random seeds from config\n",
                "torch.manual_seed(ddpg_config[\"training\"].get(\"random_seed\", 42))\n",
                "np.random.seed(ddpg_config[\"training\"].get(\"random_seed\", 42))\n",
                "\n",
                "# Create environment\n",
                "env = BalancerEnv(config_path=\"../configs/env_config.yaml\", render_mode=\"rgb_array\")\n",
                "\n",
                "# Initialize SimNet from config\n",
                "simnet = SimNet(\n",
                "    state_dim=env.observation_space.shape[0],\n",
                "    action_dim=env.action_space.shape[0],\n",
                "    hidden_dims=simnet_config[\"model\"][\"hidden_dims\"],\n",
                ").to(device)\n",
                "\n",
                "# Load the state dictionary\n",
                "simnet.load_state_dict(torch.load(\"logs/simnet_training/simnet_final.pt\", map_location=device)[\"state_dict\"])\n",
                "\n",
                "# Set the simnet in the environment\n",
                "env.simnet = simnet\n",
                "\n",
                "# Initialize trainer with config\n",
                "trainer = DDPGTrainer(env=env, config_path=\"../configs/ddpg_config.yaml\")\n",
                "\n",
                "# Print model summaries\n",
                "trainer.print_model_info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   0%|          | 2/2000 [00:00<05:16,  6.32it/s]c:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:151: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
                        "  critic_loss = torch.nn.functional.mse_loss(current_Q, target_Q)\n",
                        "Episodes:   0%|          | 9/2000 [01:38<6:04:14, 10.98s/it]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m train_config \u001b[38;5;241m=\u001b[39m ddpg_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_steps_per_episode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:226\u001b[0m, in \u001b[0;36mDDPGTrainer.train\u001b[1;34m(self, num_episodes, max_steps, batch_size, eval_freq, save_freq, log_dir)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m eval_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 226\u001b[0m     eval_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger:\n\u001b[0;32m    228\u001b[0m         logger\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m: eval_reward})\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:260\u001b[0m, in \u001b[0;36mDDPGTrainer.evaluate\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    259\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 260\u001b[0m     state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    263\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m episode_reward\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\environment\\balancer_env.py:152\u001b[0m, in \u001b[0;36mBalancerEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     s_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m     a_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorque\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# self.state = self.simnet(s_tensor, a_tensor).cpu().detach().numpy()[0]  # Not sure about this line\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimnet(s_tensor, a_tensor)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Extract training parameters from config\n",
                "train_config = ddpg_config['training']\n",
                "\n",
                "# Train agent\n",
                "history = trainer.train(\n",
                "    num_episodes=train_config['total_episodes'],\n",
                "    max_steps=train_config['max_steps_per_episode'],\n",
                "    batch_size=train_config['batch_size'],\n",
                "    eval_freq=train_config['eval_frequency'],\n",
                "    save_freq=train_config['save_frequency'],\n",
                "    log_dir=log_dir\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analysis and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training metrics\n",
                "fig = plot_training_metrics(history, save_path=log_dir / 'training_metrics.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create demonstration video\n",
                "def collect_demo_episode(max_steps=500):\n",
                "    state = env.reset()\n",
                "    states = []\n",
                "    total_reward = 0\n",
                "\n",
                "    for _ in range(max_steps):\n",
                "        action = trainer.select_action(state, training=False)\n",
                "        next_state, reward, done, info = env.step(action)\n",
                "        states.append(state)\n",
                "        total_reward += reward\n",
                "        \n",
                "        if done:\n",
                "            break\n",
                "            \n",
                "        state = next_state\n",
                "    \n",
                "    return np.array(states), total_reward\n",
                "\n",
                "# Collect several episodes and use the best one for visualization\n",
                "num_episodes = 5\n",
                "best_reward = float('-inf')\n",
                "best_states = None\n",
                "\n",
                "for _ in range(num_episodes):\n",
                "    states, reward = collect_demo_episode()\n",
                "    if reward > best_reward:\n",
                "        best_reward = reward\n",
                "        best_states = states\n",
                "\n",
                "print(f\"Best episode reward: {best_reward:.2f}\")\n",
                "\n",
                "# Create and display animation\n",
                "anim = create_episode_animation(\n",
                "    states=best_states,\n",
                "    save_path=log_dir / 'demo.mp4'\n",
                ")\n",
                "from IPython.display import HTML\n",
                "HTML(anim.to_jshtml())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Final Model with Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final model with config and training history\n",
                "torch.save({\n",
                "    'actor_state_dict': trainer.actor.state_dict(),\n",
                "    'critic_state_dict': trainer.critic.state_dict(),\n",
                "    'training_history': history,\n",
                "    'config': ddpg_config,\n",
                "    'env_config': env_config,\n",
                "    'metadata': {\n",
                "        'state_dim': env.observation_space.shape[0],\n",
                "        'action_dim': env.action_space.shape[0],\n",
                "        'max_action': float(env.action_space.high[0]),\n",
                "        'final_eval_reward': trainer.evaluate(num_episodes=10)\n",
                "    }\n",
                "}, log_dir / 'final_model.pt')\n",
                "\n",
                "print(\"Training complete! Model saved with metadata.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
