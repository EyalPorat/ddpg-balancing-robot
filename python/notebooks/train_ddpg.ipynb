{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div dir=\"rtl\">\n",
                "\n",
                "## למידת חיזוק במערכת פיזיקלית מתייצבת - אימון הסוכן\n",
                "\n",
                "**שם התלמיד:** אייל פורת  \n",
                "**שם המורה:** שי פרח  \n",
                "**תאריך:** אפריל 2025\n",
                "\n",
                "במחברת זו נאמן סוכן למידת חיזוק מסוג `DDPG (Deep Deterministic Policy Gradient)` באמצעות סביבת סימולצית הפיזיקה SimNet, אותה אימנו במחברת הייעודית.\n",
                "\n",
                "`DDPG` הוא שיפור של ארכיטקטורת `Actor-Critic` הסטנדרטית, המתאים למרחבי פעולות רציפים כמו במצערת של הרובוט.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment the following lines to run in Google Colab\n",
                "\n",
                "# %cd /content\n",
                "# !git clone https://github.com/EyalPorat/ddpg-balancing-robot.git\n",
                "# %cd ddpg-balancing-robot\n",
                "# !git checkout simpler-state-3\n",
                "# %cd /content/ddpg-balancing-robot/python/notebooks\n",
                "\n",
                "# import sys\n",
                "# sys.path.append('/content/ddpg-balancing-robot/python')  # Add the repo root to Python path"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div dir=\"rtl\">\n",
                "\n",
                "## ייבוא ספריות - Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import yaml\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from src.balancing_robot.models import Actor, Critic, ReplayBuffer, SimNet\n",
                "from src.balancing_robot.environment import BalancerEnv\n",
                "from src.balancing_robot.training import DDPGTrainer\n",
                "from src.balancing_robot.visualization import plot_training_metrics, create_episode_animation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div dir=\"rtl\">\n",
                "\n",
                "## טעינת קונפיגורציה - Configuration Loading\n",
                "\n",
                "תחילה נטען את קבצי הקונפיגורציה המכילים את פרמטרי האימון והסביבה. באמצעותם ניתן בשלוט באופי האימון מבלי לצלול לתוך הקוד.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load DDPG and environment configurations\n",
                "with open('../configs/ddpg_config.yaml', 'r') as f:\n",
                "    ddpg_config = yaml.safe_load(f)\n",
                "\n",
                "with open('../configs/env_config.yaml', 'r') as f:\n",
                "    env_config = yaml.safe_load(f)\n",
                "\n",
                "with open('../configs/simnet_config.yaml', 'r') as f:\n",
                "    simnet_config = yaml.safe_load(f)\n",
                "\n",
                "# Create log directory\n",
                "log_dir = Path('logs/ddpg_training')\n",
                "log_dir.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div dir=\"rtl\">\n",
                "\n",
                "## אתחול סביבת הסימולציה והמודלים - Initialize Environment and Models\n",
                "\n",
                "נגדיר את הסוכן על שני המודלים שלו (Actor ו-Critic) ואת הסביבה SimNet נטען מהאחסון.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "# Set random seeds from config\n",
                "torch.manual_seed(ddpg_config[\"training\"].get(\"random_seed\", 42))\n",
                "np.random.seed(ddpg_config[\"training\"].get(\"random_seed\", 42))\n",
                "\n",
                "# Create environment\n",
                "env = BalancerEnv(config_path=\"../configs/env_config.yaml\", render_mode=\"rgb_array\")\n",
                "\n",
                "# Initialize SimNet from config\n",
                "simnet = SimNet(\n",
                "    state_dim=env.observation_space.shape[0],\n",
                "    action_dim=env.action_space.shape[0],\n",
                "    hidden_dims=simnet_config[\"model\"][\"hidden_dims\"],\n",
                ").to(device)\n",
                "\n",
                "# Load the state dictionary\n",
                "simnet.load_state_dict(torch.load(\"logs/simnet_training/simnet_final.pt\", map_location=device)[\"state_dict\"])\n",
                "simnet.to(device)\n",
                "simnet.eval()  # Set to evaluation mode\n",
                "\n",
                "# Set the simnet in the environment\n",
                "env.simnet = simnet\n",
                "\n",
                "# Initialize trainer with config\n",
                "trainer = DDPGTrainer(env=env, config_path=\"../configs/ddpg_config.yaml\")\n",
                "\n",
                "# Print model summaries\n",
                "trainer.print_model_info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div dir=\"rtl\">\n",
                "\n",
                "## אימון - Training\n",
                "\n",
                "תהליך האימון בלמידת חיזוק הוא תהליך איטרטיבי, בו הסוכן מוצב בסביבה ומנסה פעולות שונות בניסיון להגיע לערך החזר מירבי.\n",
                "\n",
                "במקרה של DDPG - התיקונים של המודלים נעשים בשלבים, כאשר בתחילה מעדכנים את המבקר (Critic) באמצעות `TD-Error` (תיקון חלקי של ערך ההחזר הכולל באמצעות ערך החזר רגעי המתקבל מהסביבה) ולאחר מכן את השחקן (Actor) באמצעות `Policy Gradient` (ניסיון למקסם את ערך המבקר שקיבל הסוכן).\n",
                "\n",
                "</div>\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract training parameters from config\n",
                "train_config = ddpg_config['training']\n",
                "\n",
                "# Train agent\n",
                "history = trainer.train(\n",
                "    num_episodes=train_config['total_episodes'],\n",
                "    max_steps=train_config['max_steps_per_episode'],\n",
                "    batch_size=train_config['batch_size'],\n",
                "    eval_freq=train_config['eval_frequency'],\n",
                "    save_freq=train_config['save_frequency'],\n",
                "    log_dir=log_dir\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div dir=\"rtl\">\n",
                "\n",
                "## ניתוח תוצאות האימון - Results Analysis\n",
                "\n",
                "כמה נקודות חשובות:\n",
                "- נביט על מדדי האימון לאורך זמן. השגיאה של המבקר (Critic) יורדת עם הזמן (כך שהמבקר מכיר טוב יותר את הסביבה ואת פונקציית ההחזר).\n",
                "- ערך החיזוי של המבקר (Critic) גדל (כך שערך ההפסד של השחקן קטן).\n",
                "- אורך הפרקים גדל עם הזמן, מה שמעיד על ייצוב מוצלח.\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training metrics\n",
                "fig = plot_training_metrics(history, save_path=log_dir / 'training_metrics.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div dir=\"rtl\">\n",
                "\n",
                "## רינדור של פרק בסימולציה - Episode Rendering\n",
                "\n",
                "נביט על פרק מדומה, בו ניתן לראות את הרובוט מתייצב."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create demonstration video\n",
                "def collect_demo_episode(max_steps=500):\n",
                "    state, _ = env.reset()\n",
                "    states = []\n",
                "    actions = []\n",
                "    total_reward = 0\n",
                "\n",
                "    for _ in range(max_steps):\n",
                "        action = trainer.select_action(state, training=False)\n",
                "        next_state, reward, done, truncated, info = env.step(action)\n",
                "        states.append(state)\n",
                "        actions.append(action)\n",
                "        total_reward += reward\n",
                "        \n",
                "        if done:\n",
                "            break\n",
                "            \n",
                "        state = next_state\n",
                "    \n",
                "    return np.array(states), np.array(actions), total_reward\n",
                "\n",
                "# Collect several episodes and use the best one for visualization\n",
                "num_episodes = 5\n",
                "best_reward = float('-inf')\n",
                "best_states = None\n",
                "best_actions = None\n",
                "\n",
                "for _ in range(num_episodes):\n",
                "    states, actions, reward = collect_demo_episode()\n",
                "    if reward > best_reward:\n",
                "        best_reward = reward\n",
                "        best_states = states\n",
                "        best_actions = actions\n",
                "\n",
                "print(f\"Best episode reward: {best_reward:.2f}\")\n",
                "\n",
                "# Create and display animation\n",
                "anim = create_episode_animation(\n",
                "    states=best_states,\n",
                "    actions=best_actions,\n",
                "    save_path=log_dir / 'demo.mp4'\n",
                ")\n",
                "from IPython.display import HTML\n",
                "HTML(anim.to_jshtml())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div dir=\"rtl\">\n",
                "\n",
                "# שמירת המודל הסופי - Final Model Saving\n",
                "\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final model with config and training history\n",
                "torch.save({\n",
                "    'actor_state_dict': trainer.actor.state_dict(),\n",
                "    'critic_state_dict': trainer.critic.state_dict(),\n",
                "    'training_history': history,\n",
                "    'config': ddpg_config,\n",
                "    'env_config': env_config,\n",
                "    'metadata': {\n",
                "        'state_dim': env.observation_space.shape[0],\n",
                "        'action_dim': env.action_space.shape[0],\n",
                "        'max_action': float(env.action_space.high[0]),\n",
                "        'final_eval_reward': trainer.evaluate(num_episodes=10)\n",
                "    }\n",
                "}, log_dir / 'final_model.pt')\n",
                "\n",
                "print(\"Training complete! Model saved with metadata.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
