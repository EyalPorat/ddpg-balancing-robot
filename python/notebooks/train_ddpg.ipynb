{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DDPG Training for Balancing Robot\n",
                "\n",
                "This notebook trains a DDPG agent for the balancing robot environment using the provided configurations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import yaml\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from src.balancing_robot.models import Actor, Critic, ReplayBuffer\n",
                "from src.balancing_robot.environment import BalancerEnv\n",
                "from src.balancing_robot.training import DDPGTrainer\n",
                "from src.balancing_robot.visualization import plot_training_metrics, create_episode_animation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Configurations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load DDPG and environment configurations\n",
                "with open('../configs/ddpg_config.yaml', 'r') as f:\n",
                "    ddpg_config = yaml.safe_load(f)\n",
                "\n",
                "with open('../configs/env_config.yaml', 'r') as f:\n",
                "    env_config = yaml.safe_load(f)\n",
                "\n",
                "# Create log directory\n",
                "log_dir = Path('logs/ddpg_training')\n",
                "log_dir.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Environment and Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
                        "  gym.logger.warn(\n",
                        "c:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
                        "  gym.logger.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Actor(\n",
                        "  (network): Sequential(\n",
                        "    (0): Linear(in_features=6, out_features=8, bias=True)\n",
                        "    (1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
                        "    (2): ReLU()\n",
                        "    (3): Linear(in_features=8, out_features=8, bias=True)\n",
                        "    (4): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
                        "    (5): ReLU()\n",
                        "  )\n",
                        "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
                        ")\n",
                        "Critic(\n",
                        "  (l1): Linear(in_features=7, out_features=256, bias=True)\n",
                        "  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                        "  (hidden_layers): Sequential(\n",
                        "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
                        "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                        "    (2): ReLU()\n",
                        "  )\n",
                        "  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "# Set random seeds from config\n",
                "torch.manual_seed(ddpg_config['training'].get('random_seed', 42))\n",
                "np.random.seed(ddpg_config['training'].get('random_seed', 42))\n",
                "\n",
                "# Create environment\n",
                "env = BalancerEnv(\n",
                "    config_path='../configs/env_config.yaml',\n",
                "    render_mode='rgb_array'\n",
                ")\n",
                "\n",
                "# Initialize trainer with config\n",
                "trainer = DDPGTrainer(\n",
                "    env=env,\n",
                "    config_path='../configs/ddpg_config.yaml'\n",
                ")\n",
                "\n",
                "# Print model summaries\n",
                "trainer.print_model_info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states shape: (512, 6)\n",
                        "actions shape: (512,)\n",
                        "rewards shape: (512,)\n",
                        "next_states shape: (512, 6)\n",
                        "dones shape: (512,)\n",
                        "states shape: torch.Size([512, 6])\n",
                        "actions shape: torch.Size([512])\n",
                        "rewards shape: torch.Size([512])\n",
                        "next_states shape: torch.Size([512, 6])\n",
                        "dones shape: torch.Size([512])\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "Tensors must have same number of dimensions: got 2 and 1",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m train_config \u001b[38;5;241m=\u001b[39m ddpg_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_steps_per_episode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:227\u001b[0m, in \u001b[0;36mDDPGTrainer.train\u001b[1;34m(self, num_episodes, max_steps, batch_size, eval_freq, save_freq, log_dir)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Train if enough samples\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m--> 227\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger:\n\u001b[0;32m    229\u001b[0m         logger\u001b[38;5;241m.\u001b[39mlog(metrics)\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:160\u001b[0m, in \u001b[0;36mDDPGTrainer.train_step\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    157\u001b[0m     target_Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target(next_states, next_actions)\n\u001b[0;32m    158\u001b[0m     target_Q \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m target_Q\n\u001b[1;32m--> 160\u001b[0m current_Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(current_Q, target_Q)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\models\\critic.py:60\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: torch\u001b[38;5;241m.\u001b[39mTensor, action: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through the network.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m        Q-value tensor\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1(x)))\n\u001b[0;32m     62\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers(x)\n",
                        "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
                    ]
                }
            ],
            "source": [
                "# Extract training parameters from config\n",
                "train_config = ddpg_config['training']\n",
                "\n",
                "# Train agent\n",
                "history = trainer.train(\n",
                "    num_episodes=train_config['total_episodes'],\n",
                "    max_steps=train_config['max_steps_per_episode'],\n",
                "    batch_size=train_config['batch_size'],\n",
                "    eval_freq=train_config['eval_frequency'],\n",
                "    save_freq=train_config['save_frequency'],\n",
                "    log_dir=log_dir\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analysis and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training metrics\n",
                "fig = plot_training_metrics(history, save_path=log_dir / 'training_metrics.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create demonstration video\n",
                "def collect_demo_episode(max_steps=500):\n",
                "    state = env.reset()\n",
                "    states = []\n",
                "    total_reward = 0\n",
                "\n",
                "    for _ in range(max_steps):\n",
                "        action = trainer.select_action(state, training=False)\n",
                "        next_state, reward, done, info = env.step(action)\n",
                "        states.append(state)\n",
                "        total_reward += reward\n",
                "        \n",
                "        if done:\n",
                "            break\n",
                "            \n",
                "        state = next_state\n",
                "    \n",
                "    return np.array(states), total_reward\n",
                "\n",
                "# Collect several episodes and use the best one for visualization\n",
                "num_episodes = 5\n",
                "best_reward = float('-inf')\n",
                "best_states = None\n",
                "\n",
                "for _ in range(num_episodes):\n",
                "    states, reward = collect_demo_episode()\n",
                "    if reward > best_reward:\n",
                "        best_reward = reward\n",
                "        best_states = states\n",
                "\n",
                "print(f\"Best episode reward: {best_reward:.2f}\")\n",
                "\n",
                "# Create and display animation\n",
                "anim = create_episode_animation(\n",
                "    states=best_states,\n",
                "    save_path=log_dir / 'demo.mp4'\n",
                ")\n",
                "from IPython.display import HTML\n",
                "HTML(anim.to_jshtml())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Final Model with Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final model with config and training history\n",
                "torch.save({\n",
                "    'actor_state_dict': trainer.actor.state_dict(),\n",
                "    'critic_state_dict': trainer.critic.state_dict(),\n",
                "    'training_history': history,\n",
                "    'config': ddpg_config,\n",
                "    'env_config': env_config,\n",
                "    'metadata': {\n",
                "        'state_dim': env.observation_space.shape[0],\n",
                "        'action_dim': env.action_space.shape[0],\n",
                "        'max_action': float(env.action_space.high[0]),\n",
                "        'final_eval_reward': trainer.evaluate(num_episodes=10)\n",
                "    }\n",
                "}, log_dir / 'final_model.pt')\n",
                "\n",
                "print(\"Training complete! Model saved with metadata.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
