{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DDPG Training for Balancing Robot\n",
                "\n",
                "This notebook trains a DDPG agent for the balancing robot environment using the provided configurations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment the following lines to run in Google Colab\n",
                "\n",
                "# %cd /content\n",
                "# !git clone https://github.com/EyalPorat/ddpg-balancing-robot.git\n",
                "# %cd ddpg-balancing-robot\n",
                "# !git checkout master-organize-python-proj\n",
                "# %cd /content/ddpg-balancing-robot/python/notebooks\n",
                "\n",
                "# import sys\n",
                "# sys.path.append('/content/ddpg-balancing-robot/python')  # Add the repo root to Python path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import yaml\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from src.balancing_robot.models import Actor, Critic, ReplayBuffer, SimNet\n",
                "from src.balancing_robot.environment import BalancerEnv\n",
                "from src.balancing_robot.training import DDPGTrainer\n",
                "from src.balancing_robot.visualization import plot_training_metrics, create_episode_animation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Configurations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load DDPG and environment configurations\n",
                "with open('../configs/ddpg_config.yaml', 'r') as f:\n",
                "    ddpg_config = yaml.safe_load(f)\n",
                "\n",
                "with open('../configs/env_config.yaml', 'r') as f:\n",
                "    env_config = yaml.safe_load(f)\n",
                "\n",
                "with open('../configs/simnet_config.yaml', 'r') as f:\n",
                "    simnet_config = yaml.safe_load(f)\n",
                "\n",
                "# Create log directory\n",
                "log_dir = Path('logs/ddpg_training')\n",
                "log_dir.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Environment and Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
                        "  gym.logger.warn(\n",
                        "c:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
                        "  gym.logger.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "state_dim 6\n",
                        "action_dim 1\n",
                        "Actor(\n",
                        "  (network): Sequential(\n",
                        "    (0): Linear(in_features=6, out_features=8, bias=True)\n",
                        "    (1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
                        "    (2): ReLU()\n",
                        "    (3): Linear(in_features=8, out_features=8, bias=True)\n",
                        "    (4): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
                        "    (5): ReLU()\n",
                        "  )\n",
                        "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
                        ")\n",
                        "Critic(\n",
                        "  (l1): Linear(in_features=7, out_features=256, bias=True)\n",
                        "  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                        "  (hidden_layers): Sequential(\n",
                        "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
                        "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                        "    (2): ReLU()\n",
                        "  )\n",
                        "  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "# Set random seeds from config\n",
                "torch.manual_seed(ddpg_config[\"training\"].get(\"random_seed\", 42))\n",
                "np.random.seed(ddpg_config[\"training\"].get(\"random_seed\", 42))\n",
                "\n",
                "# Create environment\n",
                "env = BalancerEnv(config_path=\"../configs/env_config.yaml\", render_mode=\"rgb_array\")\n",
                "\n",
                "# Initialize SimNet from config\n",
                "simnet = SimNet(\n",
                "    state_dim=env.observation_space.shape[0],\n",
                "    action_dim=env.action_space.shape[0],\n",
                "    hidden_dims=simnet_config[\"model\"][\"hidden_dims\"],\n",
                ").to(device)\n",
                "\n",
                "# Load the state dictionary\n",
                "simnet.load_state_dict(torch.load(\"logs/simnet_training/simnet_final.pt\", map_location=device)[\"state_dict\"])\n",
                "\n",
                "# Set the simnet in the environment\n",
                "env.simnet = simnet\n",
                "\n",
                "# Initialize trainer with config\n",
                "trainer = DDPGTrainer(env=env, config_path=\"../configs/ddpg_config.yaml\")\n",
                "\n",
                "# Print model summaries\n",
                "trainer.print_model_info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training:   0%|          | 5/2000 [00:00<04:11,  7.94it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training:   0%|          | 7/2000 [00:02<15:02,  2.21it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training:   0%|          | 7/2000 [00:05<25:14,  1.32it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n",
                        "states torch.Size([512, 6])\n",
                        "actions torch.Size([512, 1])\n",
                        "current_Q torch.Size([512, 1])\n",
                        "target_Q torch.Size([512, 1])\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m train_config \u001b[38;5;241m=\u001b[39m ddpg_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_steps_per_episode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:226\u001b[0m, in \u001b[0;36mDDPGTrainer.train\u001b[1;34m(self, num_episodes, max_steps, batch_size, eval_freq, save_freq, log_dir)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Train if enough samples\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m--> 226\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger:\n\u001b[0;32m    228\u001b[0m         logger\u001b[38;5;241m.\u001b[39mlog(metrics)\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:169\u001b[0m, in \u001b[0;36mDDPGTrainer.train_step\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    168\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Update target networks\u001b[39;00m\n\u001b[0;32m    172\u001b[0m polyak_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:372\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m    370\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m    373\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Extract training parameters from config\n",
                "train_config = ddpg_config['training']\n",
                "\n",
                "# Train agent\n",
                "history = trainer.train(\n",
                "    num_episodes=train_config['total_episodes'],\n",
                "    max_steps=train_config['max_steps_per_episode'],\n",
                "    batch_size=train_config['batch_size'],\n",
                "    eval_freq=train_config['eval_frequency'],\n",
                "    save_freq=train_config['save_frequency'],\n",
                "    log_dir=log_dir\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analysis and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training metrics\n",
                "fig = plot_training_metrics(history, save_path=log_dir / 'training_metrics.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create demonstration video\n",
                "def collect_demo_episode(max_steps=500):\n",
                "    state, _ = env.reset()\n",
                "    states = []\n",
                "    actions = []\n",
                "    total_reward = 0\n",
                "\n",
                "    for _ in range(max_steps):\n",
                "        action = trainer.select_action(state, training=False)\n",
                "        next_state, reward, done, truncated, info = env.step(action)\n",
                "        states.append(state)\n",
                "        actions.append(action)\n",
                "        total_reward += reward\n",
                "        \n",
                "        if done:\n",
                "            break\n",
                "            \n",
                "        state = next_state\n",
                "    \n",
                "    return np.array(states), np.array(actions), total_reward\n",
                "\n",
                "# Collect several episodes and use the best one for visualization\n",
                "num_episodes = 5\n",
                "best_reward = float('-inf')\n",
                "best_states = None\n",
                "best_actions = None\n",
                "\n",
                "for _ in range(num_episodes):\n",
                "    states, actions, reward = collect_demo_episode()\n",
                "    if reward > best_reward:\n",
                "        best_reward = reward\n",
                "        best_states = states\n",
                "        best_actions = actions\n",
                "\n",
                "print(f\"Best episode reward: {best_reward:.2f}\")\n",
                "\n",
                "# Create and display animation\n",
                "anim = create_episode_animation(\n",
                "    states=best_states,\n",
                "    actions=best_actions,\n",
                "    save_path=log_dir / 'demo.mp4'\n",
                ")\n",
                "from IPython.display import HTML\n",
                "HTML(anim.to_jshtml())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Final Model with Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final model with config and training history\n",
                "torch.save({\n",
                "    'actor_state_dict': trainer.actor.state_dict(),\n",
                "    'critic_state_dict': trainer.critic.state_dict(),\n",
                "    'training_history': history,\n",
                "    'config': ddpg_config,\n",
                "    'env_config': env_config,\n",
                "    'metadata': {\n",
                "        'state_dim': env.observation_space.shape[0],\n",
                "        'action_dim': env.action_space.shape[0],\n",
                "        'max_action': float(env.action_space.high[0]),\n",
                "        'final_eval_reward': trainer.evaluate(num_episodes=10)\n",
                "    }\n",
                "}, log_dir / 'final_model.pt')\n",
                "\n",
                "print(\"Training complete! Model saved with metadata.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
