{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DDPG Training for Balancing Robot\n",
                "\n",
                "This notebook trains a DDPG agent for the balancing robot environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "from src.balancing_robot.models import Actor, Critic, ReplayBuffer\n",
                "from src.balancing_robot.environment import BalancerEnv\n",
                "from src.balancing_robot.training import DDPGTrainer\n",
                "from src.balancing_robot.visualization import plot_training_metrics, create_episode_animation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set random seeds\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "# Create environment\n",
                "env = BalancerEnv(render_mode='rgb_array')\n",
                "\n",
                "# Model parameters\n",
                "state_dim = env.observation_space.shape[0]\n",
                "action_dim = env.action_space.shape[0]\n",
                "max_action = float(env.action_space.high[0])\n",
                "\n",
                "# Initialize networks\n",
                "actor = Actor(state_dim, action_dim, max_action)\n",
                "critic = Critic(state_dim, action_dim)\n",
                "buffer = ReplayBuffer(int(1e6))\n",
                "\n",
                "# Initialize trainer\n",
                "trainer = DDPGTrainer(\n",
                "    env=env,\n",
                "    actor=actor,\n",
                "    critic=critic,\n",
                "    buffer=buffer,\n",
                "    actor_lr=1e-4,\n",
                "    critic_lr=3e-4,\n",
                "    gamma=0.99,\n",
                "    tau=0.005,\n",
                "    action_noise=0.1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training parameters\n",
                "num_episodes = 2000\n",
                "max_steps = 500\n",
                "batch_size = 512\n",
                "eval_freq = 10\n",
                "save_freq = 100\n",
                "\n",
                "# Create log directory\n",
                "log_dir = Path('logs/ddpg_training')\n",
                "log_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Train agent\n",
                "history = trainer.train(\n",
                "    num_episodes=num_episodes,\n",
                "    max_steps=max_steps,\n",
                "    batch_size=batch_size,\n",
                "    eval_freq=eval_freq,\n",
                "    save_freq=save_freq,\n",
                "    log_dir=log_dir\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training results\n",
                "fig = plot_training_metrics(history, save_path=log_dir / 'training_metrics.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create demonstration video\n",
                "state = env.reset()\n",
                "states = []\n",
                "\n",
                "for _ in range(max_steps):\n",
                "    action = trainer.select_action(state, training=False)\n",
                "    next_state, reward, done, _ = env.step(action)\n",
                "    states.append(state)\n",
                "    if done:\n",
                "        break\n",
                "    state = next_state\n",
                "\n",
                "states = np.array(states)\n",
                "anim = create_episode_animation(states, save_path=log_dir / 'demo.mp4')\n",
                "from IPython.display import HTML\n",
                "HTML(anim.to_jshtml())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}