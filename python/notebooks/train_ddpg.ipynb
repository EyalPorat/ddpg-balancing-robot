{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DDPG Training for Balancing Robot\n",
                "\n",
                "This notebook trains a DDPG agent for the balancing robot environment using the provided configurations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment the following lines to run in Google Colab\n",
                "\n",
                "# %cd /content\n",
                "# !git clone https://github.com/EyalPorat/ddpg-balancing-robot.git\n",
                "# %cd ddpg-balancing-robot\n",
                "# !git checkout master-organize-python-proj\n",
                "# %cd /content/ddpg-balancing-robot/python/notebooks\n",
                "\n",
                "# import sys\n",
                "# sys.path.append('/content/ddpg-balancing-robot/python')  # Add the repo root to Python path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import yaml\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from src.balancing_robot.models import Actor, Critic, ReplayBuffer, SimNet\n",
                "from src.balancing_robot.environment import BalancerEnv\n",
                "from src.balancing_robot.training import DDPGTrainer\n",
                "from src.balancing_robot.visualization import plot_training_metrics, create_episode_animation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Configurations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load DDPG and environment configurations\n",
                "with open('../configs/ddpg_config.yaml', 'r') as f:\n",
                "    ddpg_config = yaml.safe_load(f)\n",
                "\n",
                "with open('../configs/env_config.yaml', 'r') as f:\n",
                "    env_config = yaml.safe_load(f)\n",
                "\n",
                "with open('../configs/simnet_config.yaml', 'r') as f:\n",
                "    simnet_config = yaml.safe_load(f)\n",
                "\n",
                "# Create log directory\n",
                "log_dir = Path('logs/ddpg_training')\n",
                "log_dir.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Environment and Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
                        "  gym.logger.warn(\n",
                        "c:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
                        "  gym.logger.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Actor(\n",
                        "  (network): Sequential(\n",
                        "    (0): Linear(in_features=6, out_features=8, bias=True)\n",
                        "    (1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
                        "    (2): ReLU()\n",
                        "    (3): Linear(in_features=8, out_features=8, bias=True)\n",
                        "    (4): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
                        "    (5): ReLU()\n",
                        "  )\n",
                        "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
                        ")\n",
                        "Critic(\n",
                        "  (l1): Linear(in_features=7, out_features=256, bias=True)\n",
                        "  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                        "  (hidden_layers): Sequential(\n",
                        "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
                        "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
                        "    (2): ReLU()\n",
                        "  )\n",
                        "  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "# Set random seeds from config\n",
                "torch.manual_seed(ddpg_config[\"training\"].get(\"random_seed\", 42))\n",
                "np.random.seed(ddpg_config[\"training\"].get(\"random_seed\", 42))\n",
                "\n",
                "# Create environment\n",
                "env = BalancerEnv(config_path=\"../configs/env_config.yaml\", render_mode=\"rgb_array\")\n",
                "\n",
                "# Initialize SimNet from config\n",
                "simnet = SimNet(\n",
                "    state_dim=env.observation_space.shape[0],\n",
                "    action_dim=env.action_space.shape[0],\n",
                "    hidden_dims=simnet_config[\"model\"][\"hidden_dims\"],\n",
                ").to(device)\n",
                "\n",
                "# Load the state dictionary\n",
                "simnet.load_state_dict(torch.load(\"logs/simnet_training/simnet_final.pt\", map_location=device)[\"state_dict\"])\n",
                "\n",
                "# Set the simnet in the environment\n",
                "env.simnet = simnet\n",
                "\n",
                "# Initialize trainer with config\n",
                "trainer = DDPGTrainer(env=env, config_path=\"../configs/ddpg_config.yaml\")\n",
                "\n",
                "# Print model summaries\n",
                "trainer.print_model_info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   0%|          | 3/2000 [00:00<01:29, 22.25it/s]c:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:151: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
                        "  critic_loss = torch.nn.functional.mse_loss(current_Q, target_Q)\n",
                        "Episodes:   0%|          | 10/2000 [00:06<28:29,  1.16it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 10: Eval reward = 120.69\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   1%|          | 20/2000 [00:22<42:04,  1.28s/it]  "
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 20: Eval reward = -173.78\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   2%|▏         | 30/2000 [00:27<13:52,  2.37it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 30: Eval reward = -2391.51\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   2%|▏         | 40/2000 [00:30<09:22,  3.49it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 40: Eval reward = -2155.19\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   2%|▎         | 50/2000 [00:33<11:31,  2.82it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 50: Eval reward = -2553.63\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   3%|▎         | 60/2000 [00:37<11:32,  2.80it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 60: Eval reward = -2755.43\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   4%|▎         | 70/2000 [00:40<11:00,  2.92it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 70: Eval reward = -2195.51\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   4%|▍         | 80/2000 [00:44<12:31,  2.55it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 80: Eval reward = -2518.00\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   4%|▍         | 90/2000 [00:48<10:51,  2.93it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 90: Eval reward = -2550.93\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   5%|▌         | 100/2000 [00:51<11:27,  2.76it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 100: Eval reward = -2977.53\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   6%|▌         | 110/2000 [00:55<11:59,  2.63it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 110: Eval reward = -2588.36\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   6%|▌         | 120/2000 [00:58<11:03,  2.83it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 120: Eval reward = -2623.31\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   6%|▋         | 130/2000 [01:02<11:16,  2.76it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 130: Eval reward = -2162.74\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   7%|▋         | 140/2000 [01:06<09:58,  3.11it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 140: Eval reward = -2395.77\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   8%|▊         | 150/2000 [01:09<10:56,  2.82it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 150: Eval reward = -2282.70\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   8%|▊         | 160/2000 [01:14<16:22,  1.87it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 160: Eval reward = -1909.23\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   8%|▊         | 170/2000 [01:17<12:04,  2.53it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 170: Eval reward = -2764.92\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:   9%|▉         | 180/2000 [01:21<11:49,  2.57it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 180: Eval reward = -2361.71\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  10%|▉         | 190/2000 [01:25<10:11,  2.96it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 190: Eval reward = -2456.87\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  10%|█         | 200/2000 [01:28<09:49,  3.05it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 200: Eval reward = -2498.50\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  10%|█         | 210/2000 [01:32<12:50,  2.32it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 210: Eval reward = -2505.80\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  11%|█         | 220/2000 [01:36<09:15,  3.21it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 220: Eval reward = -2593.12\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  12%|█▏        | 230/2000 [01:41<20:55,  1.41it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 230: Eval reward = -2370.30\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  12%|█▏        | 240/2000 [01:46<11:08,  2.63it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 240: Eval reward = -2249.00\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  12%|█▎        | 250/2000 [01:49<10:21,  2.82it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 250: Eval reward = -2854.44\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  13%|█▎        | 260/2000 [01:53<11:59,  2.42it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 260: Eval reward = -1968.11\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  14%|█▎        | 270/2000 [01:57<11:58,  2.41it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 270: Eval reward = -2251.58\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  14%|█▍        | 280/2000 [02:03<17:41,  1.62it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 280: Eval reward = -2855.76\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  14%|█▍        | 290/2000 [02:07<09:54,  2.88it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 290: Eval reward = -2471.36\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  15%|█▌        | 300/2000 [02:10<10:12,  2.77it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 300: Eval reward = -2396.69\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  16%|█▌        | 310/2000 [02:14<11:14,  2.50it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 310: Eval reward = -2861.46\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  16%|█▌        | 320/2000 [02:18<09:31,  2.94it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 320: Eval reward = -2699.78\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  16%|█▋        | 330/2000 [02:22<11:54,  2.34it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 330: Eval reward = -2241.40\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  17%|█▋        | 340/2000 [02:26<09:09,  3.02it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 340: Eval reward = -2088.17\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  18%|█▊        | 350/2000 [02:30<12:29,  2.20it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 350: Eval reward = -2317.15\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  18%|█▊        | 360/2000 [02:36<14:33,  1.88it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 360: Eval reward = -2479.31\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  18%|█▊        | 370/2000 [02:41<12:56,  2.10it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 370: Eval reward = -2253.38\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  19%|█▉        | 380/2000 [02:45<11:05,  2.44it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 380: Eval reward = -3067.23\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  20%|█▉        | 390/2000 [02:49<10:22,  2.59it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 390: Eval reward = -3141.42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  20%|██        | 400/2000 [02:54<12:14,  2.18it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 400: Eval reward = -3013.25\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  20%|██        | 410/2000 [02:58<11:20,  2.33it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 410: Eval reward = -2814.45\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  21%|██        | 420/2000 [03:02<09:39,  2.72it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 420: Eval reward = -2558.83\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  22%|██▏       | 430/2000 [03:06<09:14,  2.83it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 430: Eval reward = -2379.42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  22%|██▏       | 440/2000 [03:09<09:23,  2.77it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 440: Eval reward = -2484.00\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  22%|██▎       | 450/2000 [03:13<10:02,  2.57it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 450: Eval reward = -2436.17\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  23%|██▎       | 460/2000 [03:17<09:30,  2.70it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 460: Eval reward = -2778.15\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  24%|██▎       | 470/2000 [03:21<10:37,  2.40it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 470: Eval reward = -2549.50\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  24%|██▍       | 480/2000 [03:24<09:03,  2.80it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 480: Eval reward = -2774.79\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  24%|██▍       | 490/2000 [03:30<13:54,  1.81it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 490: Eval reward = -2426.41\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  25%|██▌       | 500/2000 [03:35<10:37,  2.35it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 500: Eval reward = -2201.44\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  26%|██▌       | 510/2000 [03:39<10:38,  2.33it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 510: Eval reward = -2700.66\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  26%|██▌       | 520/2000 [03:43<11:39,  2.12it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 520: Eval reward = -2642.24\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  26%|██▋       | 530/2000 [03:46<08:00,  3.06it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 530: Eval reward = -2722.78\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  27%|██▋       | 540/2000 [03:50<08:13,  2.96it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 540: Eval reward = -2256.59\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  28%|██▊       | 550/2000 [03:54<09:37,  2.51it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 550: Eval reward = -2696.00\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  28%|██▊       | 560/2000 [03:58<07:49,  3.07it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 560: Eval reward = -2443.17\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  28%|██▊       | 570/2000 [04:02<10:21,  2.30it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 570: Eval reward = -2310.08\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  29%|██▉       | 580/2000 [04:06<09:06,  2.60it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 580: Eval reward = -2832.17\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  30%|██▉       | 590/2000 [04:10<10:24,  2.26it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 590: Eval reward = -2648.71\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  30%|███       | 600/2000 [04:15<13:39,  1.71it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 600: Eval reward = -2660.93\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  30%|███       | 610/2000 [04:19<09:05,  2.55it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 610: Eval reward = -3108.96\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  31%|███       | 620/2000 [04:24<11:00,  2.09it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 620: Eval reward = -2328.83\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  32%|███▏      | 630/2000 [04:28<09:01,  2.53it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 630: Eval reward = -2293.39\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  32%|███▏      | 640/2000 [04:33<09:20,  2.43it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 640: Eval reward = -2278.84\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  32%|███▎      | 650/2000 [04:36<08:12,  2.74it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 650: Eval reward = -2696.73\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  33%|███▎      | 660/2000 [04:40<09:11,  2.43it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 660: Eval reward = -2349.43\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  34%|███▎      | 670/2000 [04:44<08:30,  2.61it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 670: Eval reward = -2853.99\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  34%|███▍      | 680/2000 [04:48<08:05,  2.72it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 680: Eval reward = -1902.90\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  34%|███▍      | 690/2000 [04:52<08:01,  2.72it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 690: Eval reward = -2834.90\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  35%|███▌      | 700/2000 [04:57<12:48,  1.69it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 700: Eval reward = -2924.98\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  36%|███▌      | 710/2000 [05:02<10:08,  2.12it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 710: Eval reward = -2349.44\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  36%|███▌      | 720/2000 [05:06<08:24,  2.54it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 720: Eval reward = -2844.15\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  36%|███▋      | 730/2000 [05:10<07:21,  2.88it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 730: Eval reward = -3208.70\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  37%|███▋      | 740/2000 [05:14<09:51,  2.13it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 740: Eval reward = -2555.66\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  38%|███▊      | 750/2000 [05:18<08:05,  2.57it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 750: Eval reward = -3181.75\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  38%|███▊      | 760/2000 [05:22<08:06,  2.55it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 760: Eval reward = -2399.15\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Episodes:  38%|███▊      | 764/2000 [05:24<08:45,  2.35it/s]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m train_config \u001b[38;5;241m=\u001b[39m ddpg_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_steps_per_episode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:217\u001b[0m, in \u001b[0;36mDDPGTrainer.train\u001b[1;34m(self, num_episodes, max_steps, batch_size, eval_freq, save_freq, log_dir)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Train if enough samples\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m--> 217\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger:\n\u001b[0;32m    219\u001b[0m         logger\u001b[38;5;241m.\u001b[39mlog(metrics)\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\training\\ddpg_trainer.py:150\u001b[0m, in \u001b[0;36mDDPGTrainer.train_step\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    147\u001b[0m     target_Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target(next_states, next_actions)\n\u001b[0;32m    148\u001b[0m     target_Q \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m target_Q\n\u001b[1;32m--> 150\u001b[0m current_Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(current_Q, target_Q)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\Documents\\PlatformIO\\Projects\\balancing_robot\\python\\notebooks\\..\\src\\balancing_robot\\models\\critic.py:62\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state, action], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1(x)))\n\u001b[1;32m---> 62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\eyalp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Extract training parameters from config\n",
                "train_config = ddpg_config['training']\n",
                "\n",
                "# Train agent\n",
                "history = trainer.train(\n",
                "    num_episodes=train_config['total_episodes'],\n",
                "    max_steps=train_config['max_steps_per_episode'],\n",
                "    batch_size=train_config['batch_size'],\n",
                "    eval_freq=train_config['eval_frequency'],\n",
                "    save_freq=train_config['save_frequency'],\n",
                "    log_dir=log_dir\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analysis and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training metrics\n",
                "fig = plot_training_metrics(history, save_path=log_dir / 'training_metrics.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create demonstration video\n",
                "def collect_demo_episode(max_steps=500):\n",
                "    state = env.reset()\n",
                "    states = []\n",
                "    total_reward = 0\n",
                "\n",
                "    for _ in range(max_steps):\n",
                "        action = trainer.select_action(state, training=False)\n",
                "        next_state, reward, done, info = env.step(action)\n",
                "        states.append(state)\n",
                "        total_reward += reward\n",
                "        \n",
                "        if done:\n",
                "            break\n",
                "            \n",
                "        state = next_state\n",
                "    \n",
                "    return np.array(states), total_reward\n",
                "\n",
                "# Collect several episodes and use the best one for visualization\n",
                "num_episodes = 5\n",
                "best_reward = float('-inf')\n",
                "best_states = None\n",
                "\n",
                "for _ in range(num_episodes):\n",
                "    states, reward = collect_demo_episode()\n",
                "    if reward > best_reward:\n",
                "        best_reward = reward\n",
                "        best_states = states\n",
                "\n",
                "print(f\"Best episode reward: {best_reward:.2f}\")\n",
                "\n",
                "# Create and display animation\n",
                "anim = create_episode_animation(\n",
                "    states=best_states,\n",
                "    save_path=log_dir / 'demo.mp4'\n",
                ")\n",
                "from IPython.display import HTML\n",
                "HTML(anim.to_jshtml())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Final Model with Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final model with config and training history\n",
                "torch.save({\n",
                "    'actor_state_dict': trainer.actor.state_dict(),\n",
                "    'critic_state_dict': trainer.critic.state_dict(),\n",
                "    'training_history': history,\n",
                "    'config': ddpg_config,\n",
                "    'env_config': env_config,\n",
                "    'metadata': {\n",
                "        'state_dim': env.observation_space.shape[0],\n",
                "        'action_dim': env.action_space.shape[0],\n",
                "        'max_action': float(env.action_space.high[0]),\n",
                "        'final_eval_reward': trainer.evaluate(num_episodes=10)\n",
                "    }\n",
                "}, log_dir / 'final_model.pt')\n",
                "\n",
                "print(\"Training complete! Model saved with metadata.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
